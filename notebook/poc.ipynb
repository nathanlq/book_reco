{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c889cf5-3558-4399-bb2c-13decbeae879",
   "metadata": {},
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1578784-7ff5-47f5-9e32-ed8967a64196",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import unidecode\n",
    "import re\n",
    "\n",
    "tqdm.pandas(desc=\"Processing\")\n",
    "\n",
    "df = pd.read_parquet('../data/raw_data.parquet')\n",
    "\n",
    "with open('../data/stop_words_french.txt', 'r', encoding='utf-8') as file:\n",
    "    french_stop_words = [line.strip() for line in file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e76753eb-6efb-464c-9182-2a18f5b14b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "exploded_labels = df['labels'].explode()\n",
    "blacklisted_labels =  ['Accueil', 'Collège', 'Lycée', 'Livres', 'Littérature']\n",
    "\n",
    "def normalize_text(text):\n",
    "    if isinstance(text, str):\n",
    "        text = text.lower()\n",
    "        text = unidecode.unidecode(text)\n",
    "        text = re.sub(r'\\s+', '_', text).strip('_')\n",
    "    return text\n",
    "\n",
    "def clean_label(label):\n",
    "    label = label.strip()\n",
    "    if label in blacklisted_labels:\n",
    "        return None\n",
    "    if not label:\n",
    "        return None\n",
    "    if re.search(r'[0-9]', label) or re.search(r'[\\n\\t]', label):\n",
    "        return None\n",
    "    return normalize_text(label)\n",
    "\n",
    "cleaned_labels = exploded_labels.apply(clean_label).dropna()\n",
    "\n",
    "cleaned_labels_grouped = cleaned_labels.groupby(level=0).agg(list)\n",
    "\n",
    "df['labels'] = cleaned_labels_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14634280-ef58-49cb-8e0b-c25c3271c442",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_info = pd.json_normalize(df['information'])\n",
    "\n",
    "df = pd.concat([df.drop(columns=['information']), df_info], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1c0784c-e307-4164-8a55-cf1db2b71708",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Date de parution'] = pd.to_datetime(df['Date de parution'], format='%d/%m/%Y')\n",
    "\n",
    "df['Nb. de pages'] = df['Nb. de pages'].str.extract(r'(\\d+)').astype(float).fillna(-1).astype(int)\n",
    "\n",
    "df['Poids'] = df['Poids'].str.extract(r'([\\d.]+)').astype(float)\n",
    "\n",
    "df['EAN'] = df['EAN'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "731f2118-81de-4be8-ba8c-c929496fc953",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = ['author', 'Collection', 'Editeur', 'Format', 'Présentation']\n",
    "\n",
    "for column in categorical_columns:\n",
    "    df[column] = df[column].apply(normalize_text)\n",
    "\n",
    "for column in categorical_columns:\n",
    "    df[column] = df[column].astype('category')\n",
    "\n",
    "for column in categorical_columns:\n",
    "    df[column + '_label'] = df[column].cat.codes\n",
    "\n",
    "df.columns = [normalize_text(col) for col in df.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "afed3e44-800b-4f0e-9849-33a297a7a4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "dimensions_pattern = r'(\\d+,\\d+) cm × (\\d+,\\d+) cm × (\\d+,\\d+) cm'\n",
    "df[['width', 'height', 'depth']] = df['dimensions'].str.extract(dimensions_pattern)\n",
    "\n",
    "df['width'] = df['width'].str.replace(',', '.').astype(float)\n",
    "df['height'] = df['height'].str.replace(',', '.').astype(float)\n",
    "df['depth'] = df['depth'].str.replace(',', '.').astype(float)\n",
    "\n",
    "df.drop(columns=['dimensions'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed41aee2-f9da-4a72-a2d7-97283521a67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_parquet('../data/cleaned_data.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e2f1b7-53be-454a-9e67-28919c14cf5b",
   "metadata": {},
   "source": [
    "### Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87829d62-dd58-4456-b0ea-ed62dd1dea21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import CamembertModel, CamembertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "161e8a6e-bc7c-4dea-9d76-db6783f6cad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████| 3323/3323 [05:50<00:00,  9.49it/s]\n",
      "100%|██████████████████████████████████████████████████| 3323/3323 [01:53<00:00, 29.34it/s]\n"
     ]
    }
   ],
   "source": [
    "model_name = 'camembert-base'\n",
    "tokenizer = CamembertTokenizer.from_pretrained(model_name)\n",
    "model = CamembertModel.from_pretrained(model_name)\n",
    "\n",
    "def get_embedding(text, tokenizer, model, max_length):\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=max_length)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    embedding = outputs.last_hidden_state[:, 0, :].numpy()\n",
    "    return embedding\n",
    "\n",
    "def apply_embedding(column, tokenizer, model, max_length):\n",
    "    tqdm.pandas()\n",
    "    return column.progress_apply(lambda x: get_embedding(x, tokenizer, model, max_length))\n",
    "\n",
    "df['resume_embedding'] = apply_embedding(df['resume'], tokenizer, model, 512)\n",
    "df['product_title_embedding'] = apply_embedding(df['product_title'], tokenizer, model, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cbaffb2e-e5ca-4aff-8c5c-45a212dad748",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['resume_embedding'] = df['resume_embedding'].apply(lambda x: x.flatten().tolist())\n",
    "df['product_title_embedding'] = df['product_title_embedding'].apply(lambda x: x.flatten().tolist())\n",
    "\n",
    "df.to_parquet('../data/vectorized_data.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6119ea-965d-4bdb-bbdb-216a81fc171a",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6395f3bf-2a2b-4485-9b08-3bd4efc7265c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cc3cf475-7b9f-472c-af10-ad64c6fa3503",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nathan/Documents/Perso/self_hosted/book_reco/venv/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['quelqu'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "combined_text = df['resume'].tolist() + df['product_title'].tolist()\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words=french_stop_words, max_features=1024)\n",
    "\n",
    "tfidf_vectorizer.fit(combined_text)\n",
    "\n",
    "def apply_tfidf(column, vectorizer):\n",
    "    tqdm.pandas()\n",
    "    tfidf_matrix = vectorizer.transform(column)\n",
    "    return tfidf_matrix.toarray().tolist()\n",
    "\n",
    "df['resume_tfidf'] = apply_tfidf(df['resume'], tfidf_vectorizer)\n",
    "df['product_title_tfidf'] = apply_tfidf(df['product_title'], tfidf_vectorizer)\n",
    "\n",
    "df.to_parquet('../data/tfidf_data.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e111164c-024b-4987-a9d9-1a4375220dbb",
   "metadata": {},
   "source": [
    "### Recommandations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1f6c2e16-3a85-4beb-89ed-5f1d9eb54f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "904ce7ca-9225-43af-98c5-a9a5af595843",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_similarity(embeddings):\n",
    "    similarity_matrix = cosine_similarity(embeddings)\n",
    "    return similarity_matrix\n",
    "\n",
    "def get_top_n_similar_items(index, similarity_matrix, n=5):\n",
    "    similar_indices = np.argsort(similarity_matrix[index])[::-1][1:n+1]\n",
    "    return similar_indices\n",
    "\n",
    "def compute_combined_similarity(df, embedding_weights, categorical_weights, labels_weight, n=5):\n",
    "    resume_embeddings = np.array(df['resume_embedding'].tolist())\n",
    "    product_title_embeddings = np.array(df['product_title_embedding'].tolist())\n",
    "    resume_tfidf = np.array(df['resume_tfidf'].tolist())\n",
    "    product_title_tfidf = np.array(df['product_title_tfidf'].tolist())\n",
    "\n",
    "    resume_similarity_matrix = compute_similarity(resume_embeddings)\n",
    "    product_title_similarity_matrix = compute_similarity(product_title_embeddings)\n",
    "    resume_tfidf_similarity_matrix = compute_similarity(resume_tfidf)\n",
    "    product_title_tfidf_similarity_matrix = compute_similarity(product_title_tfidf)\n",
    "\n",
    "    scaled_resume_similarity_matrix = resume_similarity_matrix * embedding_weights['resume']\n",
    "    scaled_product_title_similarity_matrix = product_title_similarity_matrix * embedding_weights['product_title']\n",
    "    scaled_resume_tfidf_similarity_matrix = resume_tfidf_similarity_matrix * embedding_weights['resume_tfidf']\n",
    "    scaled_product_title_tfidf_similarity_matrix = product_title_tfidf_similarity_matrix * embedding_weights['product_title_tfidf']\n",
    "\n",
    "    combined_similarity_matrix = (scaled_resume_similarity_matrix +\n",
    "                                  scaled_product_title_similarity_matrix +\n",
    "                                  scaled_resume_tfidf_similarity_matrix +\n",
    "                                  scaled_product_title_tfidf_similarity_matrix)\n",
    "\n",
    "    categorical_columns_with_labels = ['author_label', 'collection_label', 'editeur_label']\n",
    "    categorical_labels = df[categorical_columns_with_labels].values\n",
    "\n",
    "    encoder = OneHotEncoder(sparse_output=False)\n",
    "    categorical_labels_one_hot = encoder.fit_transform(categorical_labels)\n",
    "\n",
    "    num_categories = [len(encoder.categories_[i]) for i in range(len(categorical_columns_with_labels))]\n",
    "\n",
    "    weight_matrix = np.concatenate([np.full(num_cat, weight) for num_cat, weight in zip(num_categories, categorical_weights.values())])\n",
    "\n",
    "    scaled_categorical_labels = categorical_labels_one_hot * weight_matrix\n",
    "\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    labels_binary_matrix = mlb.fit_transform(df['labels'])\n",
    "    scaled_labels_binary_matrix = labels_binary_matrix * labels_weight\n",
    "\n",
    "    combined_features = np.hstack((resume_embeddings, product_title_embeddings, resume_tfidf, product_title_tfidf, scaled_categorical_labels, scaled_labels_binary_matrix))\n",
    "\n",
    "    combined_similarity_matrix = compute_similarity(combined_features)\n",
    "\n",
    "    top_n_similar_items = get_top_n_similar_items(0, combined_similarity_matrix, n=n)\n",
    "\n",
    "    df['combined_vector'] = combined_features.tolist()\n",
    "\n",
    "    columns_to_drop = ['resume_embedding', 'product_title_embedding', 'resume_tfidf', 'product_title_tfidf'] + [ col for col in df.columns if '_label' in col ]\n",
    "    df_cleaned = df.drop(columns=columns_to_drop)\n",
    "\n",
    "    return df_cleaned, top_n_similar_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fc3f5c74-9e22-4e68-b3ae-8b15efbc5e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_weights = {\n",
    "    'resume': 0.3,\n",
    "    'product_title': 0.3,\n",
    "    'resume_tfidf': 0.2,\n",
    "    'product_title_tfidf': 0.2\n",
    "}\n",
    "\n",
    "categorical_weights = {\n",
    "    'author_label': 0.5,\n",
    "    'collection_label': 0.3,\n",
    "    'editeur_label': 0.2\n",
    "}\n",
    "\n",
    "labels_weight = 1\n",
    "\n",
    "df_cleaned, top_n_similar_items = compute_combined_similarity(df, embedding_weights, categorical_weights, labels_weight, n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9db9f560-0b24-4b5c-8172-88e14a71761e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned.to_parquet('../data/combined_data.parquet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
